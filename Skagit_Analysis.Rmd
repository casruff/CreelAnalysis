---
title: Skagit Creel Analysis
author: Thomas Buehrens, Kale Bentley, Andrew Fowler & Amy Edwards
output:
  html_document:
    fig_caption: yes
    theme: cerulean
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

***

This document was generated on `r format(Sys.time(), '%m/%d/%Y')`.

***


```{r set_options, echo = FALSE, message = FALSE}
options(width = 100)
knitr::opts_chunk$set(message = FALSE)
set.seed(123)
```
# Purpose
The purpose of this document is to record the steps and code necessary to reproduce the Skagit steelhead fishery creel analysis for 2021.

# Requirements
All analyses require R software [**(link)**](https://cran.r-project.org/) (v3.4.3) for data retrieval, data processing, and summarizing model results, and Stan software [**(link)**](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started/) for Hamiltonian Monte Carlo (HMC) simulation. For Stan to work, rtools must also be installed: [**(link)**](https://cran.r-project.org/bin/windows/Rtools/).

# Functions
We also need a couple of helper functions which we will load from the functions folder, which we will load using sapply
```{r load_funcs, results = "hide"}
wd_functions<-"functions"
sapply(FUN = source, paste(wd_functions, list.files(wd_functions), sep="/"))
```

# Packages
In addition to purrr, we also need a few packages that are not included with the base installation of R, so we begin by installing them (if necessary) and then loading them.
```{r load_pkgs, message = FALSE, warning = FALSE}
#===============================================
# Load packages, install and load if not already
#===============================================
using("timeDate",
      "plyr",
      "tidyverse",
      "rstan",
      "RColorBrewer",
      "readxl",
      "readr",
      "ggplot2",
      "tinytex",
      "here", 
      "lubridate", 
      "devtools",
      "xlsx",
      "cowplot",
      "ggpubr", 
      "chron",
      "suncalc", 
      "shinystan",
      "loo", 
      "data.table",
      "RColorBrewer",
      "reshape2",
      "MASS", 
      "kableExtra"
      )
```
# User inputs
```{r user_inputs,message=FALSE, warning=FALSE}
#======================================================
# Specify relative working directories for sub-folders
#======================================================
wd_LUTs <-"lookup tables"       # Location of look-up tables (maybe could be merged with data files??)
wd_data   <-"data"              # Location where data files are stored
wd_source_files<-"source files" # Location of source file (code working "behind the scenes")
wd_models  <-"models"           # Location of model files 
wd_outputs <-"results"          # Location of saved output (summary figures/tables and model results)

#======================================================
# Specify names of .csv data files
#======================================================
effort_file_name <-   "03_Effort_dat - 2019_Skagit_creel_JSH_thru_4-30-19.csv"
interview_file_name <-"03_Interview-dat_2019-Skagit_JSH_thru_4-30-2019.csv"
effort_xwalk_filename<-"02_Crosswalk_Table_for_Index_TieIn_Sections_2019-01-10.csv"
river_loc_filename<-"02_River.Locations_2019-01-07.csv"
creel_models_filename<-"02_Creel_Models_2021-01-20.csv"

#======================================================
# Denote data of interest (used to filter data below)
#======================================================
# Specify filter type(s) to extract data by (Enter "Y" or "N")
  by.Year<-      "N" # If "Y", will filter by full calendar year(s) (Jan. 1 - Dec. 31)
  by.YearGroup<- "N" # If "Y", will filter by a "Year Group", which go from May 1st Yr1 - April 30 Yr2
  by.Season<-    "N" # If "Y", will filter by "season", which is either Summer (May 1st - Oct 31st) or Winter (Nov. 1 - April 30)
  by.StreamName<-"Y" # If "Y", will filter by stream name
  by.Date<-      "N" # If "Y", will filter by a date range
  
# Specify date ranges for "Year Groups" and "Seasons"
  YearBegin<-  121 # day of year a "YearGroup" begins (FYI - 121 = May 1st in a non-leap year)
  summerBegin<-121 
  summerEnd<-  304 # FYI - 304 = Oct. 31st (in a non-leap year)
  winterBegin<-305 
  winterEnd<-  120 

# Specify filter unit(s)
  YearGroup.of.Interest<- c("2017-2018") 
  Season.of.Interest<-    c("Winter") 
  Year.of.Interest<-      c("2017") 
  StreamName.of.Interest<-c("Skagit")
  Begin.Date<-            c("2016-05-01") #Format must be "yyyy-mm-dd"
  End.Date<-              c("2017-03-31") #Format must be "yyyy-mm-dd"
  
#======================================================
# Denote catch group of interest (species_origin_fate) 
#======================================================  
  catch.group.of.interest<-c("SH_W_R") 

#======================================================
# Identify dates when fishery was closed by section
#======================================================     
  total.closed.dates<-0 # Total number of dates that at least one section of the river was closed (i.e., there should not have been any effort or catch)
  
# NOTE: if "total.closed.dates" >0, use the following format to enter closure dates and section, where :
      # the first column is the list of individual dates (by row) the fishery was closed date 
      # the number of additional columns equals the number of "final" sections based on "final.effort.section.xwalk" 
      # the enter the following values below each section:
      # Enter 1 if the section was open and enter 0 if the section was closed  
    
                           #    Date   , Section-1, Section-2
    closed.Dates.Sections<-c("2019-02-11",     0,         0 ) 
```

# Data Preparation
```{r data_prep,message=FALSE, warning=FALSE,results = "hide"}
# Load LUTs
  source(paste0(wd_source_files, "/Load_LUTs.R"))

# Load creel data and format
  source(paste0(wd_source_files, "/Import_Skagit_Creel_Data_and_Format.R"))

# Extract data of interest and format 
  ## add code that shows options for filtering data by date/year/season/location
  source(paste0(wd_source_files, "/05_Extract_Data_of_Interest_and_Calculate_Fields_2019-04-08.R"))  

#Run source summary file 
  ## add code that shows options for "catch groups"
  source(paste0(wd_source_files, "/06_Summarize_Effort_and_Catch_Data_for_TimeSeries_Model_2019-04-23.R"))   

##KB note: I will work on updating the code in the "05" and "06" file at some point soon; also, creating a "Import and format" file for data that is from our creel database
```

# Run Analysis
```{r run_analysis,results = "hide"} 
#message=FALSE, warning=FALSE
#=======
#note for editing: any ner priors need to go here, also in "prepare data" and in "summarize inputs"
#=======

# Denote whether you want to run a new model or load "saved" results from a previous model run
  model_source<-c("load_saved")  #enter either "run_new" or "load_saved"

# Assign a "Model_Run" number (if model_source == run_new, results will be saved to a new sub-folder; if model_source == load_saved, previous model results will be loaded)
  Model_Run<-1 #Enter numeric number (NOTE: be careful not to over-write previous models runs by entering a number that's already been used)

# Denote which creel model you want to run
  creel_models[,1:3] #model summary table
  model_number<-c(2)
  
# Specify time period to stratify data by - day vs. week 
  model_period<-c("day") #enter "day" or "week"
  
# Specify parameter values for model priors
  value_cauchyDF_sigma_eps_C = 1 # the hyperhyper scale (degrees of freedom) parameter in the hyperprior distribution sigma_eps_C; default = 1  
  value_cauchyDF_sigma_eps_E = 1 # the hyperhyper scale (degrees of freedom) parameter in the hyperprior distribution sigma_eps_E; default = 1  
  value_cauchyDF_sigma_r_E = 1  # the hyperhyper scale (degrees of freedom) parameter in the hyperprior distribution sigma_r_E; default = 1  
  value_cauchyDF_sigma_r_C = 1  # the hyperhyper scale (degrees of freedom) parameter in the hyperprior distribution sigma_r_C; default = 1  
  value_normal_sigma_omega_C_0 = 1  #the SD hyperparameter in the prior distribution omega_C_0; normal sd (log-space); default = 1   
  value_normal_sigma_omega_E_0 =  3 # the SD hyperparameter in the prior distribution omega_E_0; normal sd (log-space);; default = 3  
  value_lognormal_sigma_b = 1 # the SD hyperparameter in the prior distribution b; default = 1  
  value_normal_sigma_B1 = 5 # the SD hyperparameter in the prior distribution B1; default = 5  
  value_normal_mu_mu_C = log(0.02) # the mean hyperparameter in the prior distribution mu_C; median (log-space); default = 0.02 (was originally  0.05) 
  value_normal_sigma_mu_C = 1.5 # the SD hyperparameter in the prior distribution mu_C; normal sd (log-space); default = 1.5 (was originally 5)
  value_normal_mu_mu_E = log(15) # the mean hyperparameter in the prior distribution mu_E; median effort (log-space); default = 15 
  value_normal_sigma_mu_E = 2  # the SD hyperparameter in the prior distribution mu_E; normal sd (log-space); default = 2 (was originally 5) 
  value_betashape_phi_E_scaled = 1 # the rate (alpha) and shape (beta) hyperparameters in phi_E_scaled; default = 1 (i.e., beta(1,1) which is uniform), alternative beta(2,2) 
  value_betashape_phi_C_scaled = 1 # the rate (alpha) and shape (beta) hyperparameters in phi_C_scaled; default = 1 (i.e., beta(1,1) which is uniform), alternative beta(2,2)
  value_cauchyDF_sigma_mu_C = 1       # the hyperhyper SD parameter in the hyperprior distribution sigma_mu_C
  value_cauchyDF_sigma_mu_E = 1       # the hyperhyper SD parameter in the hyperprior distribution sigma_mu_E

# Specific Stan model arguments
  n_chain<-4        # set the number of Markov chains. The default is 4.
  n_iter<-200        # set the number of iterations for each chain (including warmup). The default is 2000.
  n_cores<-4         # set the number of cores to use when executing the chains in parallel. The defaults is 1. NOTE: Stan manual recommends setting the mc.cores option to be as many processors as the hardware and RAM allow (up to the number of chains).
  n_warmup<-100    # set the length of warm-up (aka burn-in) iterations per chain.  The default is n_iter/2.  
  n_thin<-1          # set the thinning rate (aka, the period for saving samples). The default is 1, which is usually the recommended value.
  adapt_delta<-0.8  # set adapt delta, which is the target average proposal acceptance probability during Stan's adaptation period. Value between 0 and 1. The default is 0.8. Increasing it will force stan to take smaller steps (and thus run time will be longer). 
  max_treedepth<-10 # set the max tree depth; default is 8; NOTE: this sets the max depth of tree used by NUTS during each iteration; warnings about hitting the maximum treedepth is an efficiency concern

# Create sub-folders for output (if they don't already exist)
    source(paste0(wd_source_files, "/Create_output_subfolder.R"), print.eval = TRUE)

# Run source code to prepare data for model
    source(paste0(wd_source_files, "/Prepare_Data_For_Model.R "))   

# Run source code to generate creel estimates
  source(paste0(wd_source_files, "/RunNew_or_LoadSaved_Creel_Model.R"))
  
# Generate summaries of model inputs and outputs
  if(model_source == "run_new"){  source(paste0(wd_source_files, "/Summarize_Model_Inputs_and_Outputs.R"))}  
```

# Summarize Results
```{r summarize_results,message=FALSE, warning=FALSE,results = "hide"}

#convergence diagnostics
  launch_diagnostics<-c("No") #Enter "Yes" to launch ShinyShin diagnostics
  if(launch_diagnostics=="Yes"){launch_shinystan(output$res_stan)} 

# generate plots and tables of creel estimates 
  source(paste0(wd_source_files, "/Generate_Summaries_of_Creel_Estimates.R")) 
    
# KB note: update so table/plots of results are shown in PDF document
```


# Reproducing this pdf or html page
In order to reproduce this pdf or html page you need to have a LaTex application installed. Running this snippet of code will automatically install tinytex on your machine so you can render pdfs and html:
```{r install_tinytex,message=FALSE, warning=FALSE}
#tinytex::install_tinytex()
#tinytex::tlmgr_install("multirow")
#uninstall_tinytex(force = FALSE, dir = tinytex_root())
```
# Results Table: Summary of Effort and Catch
```{r run_resid_analysis, message = FALSE, warning = FALSE,results = "asis",include=TRUE}
results<-read_csv(file.path("results",catch.group.of.interest,paste0("Run_",Model_Run),"summarized_estimates",paste("Summary_Total_Catch_and_Effort",catch.group.of.interest,paste0("Run_",Model_Run),".csv",sep="_")))%>%
  dplyr::rename(Variable=X1)%>%
  kbl(caption = "Table 1. Total Catch and Effort ",digits =1)%>%
  kable_classic(full_width = F, html_font = "Cambria")
print(results)
```

# Results Figures: Summary of Effort, CPUE, and Catch
```{r, include=TRUE, fig.align="center", fig.cap=c("Total Catch and Total Effort", "Daily Effort", "Daily Catch", "Daily CPUE"),out.extra=c('page=1', 'page=2','page=3','page=4'),  echo=FALSE}
# plots<-paste(filepath_modelestimates, paste("BSS creel model summary plots", catch.group.of.interest, paste0("Run_", Model_Run),".pdf", sep="_"), sep="/")
# knitr::include_graphics(rep(plots,4))
```

```{r, include=TRUE, fig.align="center", fig.cap=c("Figure 1. Season total catch and effort. Dashed lines show posterior medians 95% CI"),echo=FALSE}
season_results<-data.frame(res$C_sum,res$E_sum)%>%
  mutate(iter=row_number())%>%
  rename(`Season Total Catch`=res.C_sum,`Season Total Effort`=res.E_sum)%>%
  pivot_longer(names_to = "Parameter",values_to="value",cols=c(`Season Total Catch`,`Season Total Effort`))

ggplot(season_results,aes(x=value,fill=Parameter))+
  facet_wrap(~Parameter,ncol=1,  scales = 'free')+
  theme_bw()+
  geom_density()+
  geom_vline(season_results%>%group_by(Parameter)%>%summarise(value = quantile(value, c(0.025, 0.5, 0.975)), q = c(0.025, 0.5, 0.975)),mapping=aes(xintercept=value,group=Parameter),linetype="dashed")+
  theme(legend.title = element_blank())

```

```{r, include=TRUE, fig.align="center", fig.cap=c("Figure 2. Daily catch. Lines are posterior medians and shading shows 95% CI"),echo=FALSE}
ggplot(Catch.summary,aes(x=as.Date(Date),y=Median,col=as.factor(Gear)))+
  facet_wrap(~Section,ncol=1)+
  geom_line(size=1.2)+
  ylab(paste0("Catch - ",catch.group.of.interest," (fish)"))+
  xlab("")+
  scale_x_date(date_labels = "%b-%d",date_breaks = "weeks")+
  geom_ribbon(aes(ymin=l95, ymax=u95,fill=as.factor(Gear)),alpha=0.2,col=NA)+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 90),legend.title = element_blank())
```

```{r, include=TRUE, fig.align="center", fig.cap=c("Figure 3. Daily effort. Lines are posterior medians and shading shows 95% CI"), echo=FALSE}
ggplot(Effort.summary,aes(x=as.Date(Date),y=Median,col=as.factor(Gear)))+
  facet_wrap(~Section,ncol=1)+
  geom_line(size=1.2)+
  ylab("Angling Effort (hrs)")+
  xlab("")+
  scale_x_date(date_labels = "%b-%d",date_breaks = "weeks")+
  geom_ribbon(aes(ymin=l95, ymax=u95,fill=as.factor(Gear)),alpha=0.2,col=NA)+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 90),legend.title = element_blank())
```

```{r, include=TRUE, fig.align="center", fig.cap=c("Figure 4. Daily CPUE. Lines are posterior medians and shading shows 95% CI"),echo=FALSE}
ggplot(CPUE.summary,aes(x=as.Date(Date),y=Median,col=as.factor(Gear)))+
  facet_wrap(~Section,ncol=1)+
  geom_line(size=1.2)+
  ylab("Catch Per Unit Effort (fish/hrs)")+
  xlab("")+
  scale_x_date(date_labels = "%b-%d",date_breaks = "weeks")+
  geom_ribbon(aes(ymin=l95, ymax=u95,fill=as.factor(Gear)),alpha=0.2,col=NA)+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 90),legend.title = element_blank())
```

# Fishery Impacts Relative to RMP Limits
Now we will compare the estimated catch, assuming 10% C&R mortality, with the forecasted run size. 

First, the run size forecast is below:
```{r include=TRUE, fig.align="center", fig.cap=c("Figure 5. Runsize Forecast"),echo=FALSE}
runsize<-read_csv("https://raw.github.com/tbuehrens/Skagit-River-Steelhead-Forecast/master/analysis/cache/ensemble_forecast_posterior.csv")
ggplot(data=runsize,aes(x=ensemble_forecast_posterior))+
  geom_density(fill="forest green")+
  theme_bw()
runsize%>%
  summarise(`Run Size` = quantile(ensemble_forecast_posterior, c(0.025, 0.25, 0.5,0.75, 0.975)), q = c(0.025,0.25, 0.5,0.75, 0.975))%>%
  dplyr::rename(Quantile=q)%>%
  kbl(caption = "Table 2. Runsize ",digits =3)%>%
  kable_classic(full_width = F, html_font = "Cambria")
```

Second, the allowable harvest rates in the RMP are:
```{r message = FALSE, warning = FALSE,results = "asis",include=TRUE}
hcr<-read_csv(file.path("data","hcr.csv"))
hcr%>%mutate(MaxRunsize=format(MaxRunsize, scientific = FALSE))%>%
  rename(`Exploitation Rate` = ER)%>%
  kbl(caption = "Table 3. Allowable Harvest Rates, Skagit RMP ",digits =3)%>%
  kable_classic(full_width = F, html_font = "Cambria")
```

Third, We will calculate the allowable harvest by combining the forecast uncertainty with the harvest rate matrix. We will then plot the probability that the allowable harvest will exceed a particular value, factoring in the uncertainty in the run size based on the preseason forecast:
```{r include=TRUE, fig.align="center", fig.cap=c("Figure 6. Probability that the allowable harvest exceeds a particular number of fish based on the RMP"),echo=FALSE}
ER_func<-function(hcr,runsize){
  ER<-c(NULL)
  indexes<-c(NULL)
  for(i in 1:nrow(hcr)){
    runsize=round(runsize)
    min<-hcr$MinRunsize[i]
    max<-hcr$MaxRunsize[i]
    ind<-which(min <= runsize & runsize <= max)
    indexes<-c(indexes,ind)
    ER<-c(ER,rep(hcr$ER[i],length(ind)))
    ERdat<-data.frame(indexes,ER)
  }
 return(ERdat)
}
ER<-ER_func(hcr=hcr,runsize=runsize)
AH<-data.frame(ER$ER,runsize)
AH$AH<-AH$ER.ER*AH$ensemble_forecast_posterior
AH$Exceedence<-rev(sort(percent_rank(AH$AH)))
#hist(AH$AH,breaks=seq(0,max(AH$AH)*1.1,100))

breaks <- 10^(-10:10)
minor_breaks <- rep(1:9, 21)*(10^rep(-10:10, each=9))
ggplot(data=AH,aes(x=AH,y=Exceedence))+
  geom_line(size=1.25)+
  theme_bw()+
  scale_x_log10(breaks = breaks, minor_breaks = minor_breaks)+
  annotation_logticks(base = 10,sides = "bl")+
  coord_equal() +
  ylab("Probability of Exceedance")+
  xlab("Allowable Co-manager Harvest")
```

Finally, we will compare the sport fishery harvest with the allowable harvest to estimate the probability that the allowable harvest has been exceeded:

```{r include=TRUE, fig.align="center", fig.cap=c("Figure 7. Probability that the sport fishery harvest exceeds the allowable harvest under the RMP. Vertical lines denote 50% of the allowable harvest, which is shared between the state and tribes, and 100% of the allowable harvest"),echo=FALSE}
#H<-res$C_sum*0.1
#H<-rlnorm(1000,log(median(runsize$ensemble_forecast_posterior*0.1)),0.1)
#H<-rlnorm(1000,log(215),0.1)
H<-rlnorm(1000,log(76),0.1)
PAH<-(sample(H,10000,replace = T)/sample(AH$AH,10000,replace = T))*100
PAHdat<-data.frame(PAH,percent_rank(-PAH))%>%rename(Exceedence="percent_rank..PAH.")
ggplot(data=PAHdat,aes(x=PAH,y=Exceedence))+
  geom_line(size=1.25)+
  theme_bw()+
  ylab("Probability of Exceedence")+
  xlab("% of Allowable Co-Manager Harvest")+
  geom_vline(xintercept=50)+
  geom_vline(xintercept=100,col="red")
```


